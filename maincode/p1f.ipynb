{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73f9198-d0c4-4abd-9574-9a38a21b6092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anuval Mohan\n",
      "[nltk_data]     sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anuval Mohan\n",
      "[nltk_data]     sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/AI-Projects/FinalProject/P1/Lifestyle-Related Diseases.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m\n\u001b[0;32m     27\u001b[0m SUPPORTED_LANGUAGES \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpanish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mte\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Added Telugu\u001b[39;00m\n\u001b[0;32m     41\u001b[0m }\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Load all the datasets into a list of DataFrames\u001b[39;00m\n\u001b[0;32m     44\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/AI-Projects/FinalProject/P1/Lifestyle-Related Diseases.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     46\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/disease_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     47\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/Environmental Diseases.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     48\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/Idiopathic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     49\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/Neoplastic Diseases.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     50\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/non-infectious diseases_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     51\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/Nutritional Diseases.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     52\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/Psychiatric and Neurological Disorders.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     53\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AI-Projects/FinalProject/P1/Rare Diseases.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m ]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Combine all datasets into one DataFrame for easier searching\u001b[39;00m\n\u001b[0;32m     57\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(datasets, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/AI-Projects/FinalProject/P1/Lifestyle-Related Diseases.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of supported languages and their codes\n",
    "SUPPORTED_LANGUAGES = {\n",
    "    'en': 'English',\n",
    "    'es': 'Spanish',\n",
    "    'fr': 'French',\n",
    "    'de': 'German',\n",
    "    'zh-cn': 'Chinese (Simplified)',\n",
    "    'ja': 'Japanese',\n",
    "    'ko': 'Korean',\n",
    "    'it': 'Italian',\n",
    "    'pt': 'Portuguese',\n",
    "    'ru': 'Russian',\n",
    "    'ar': 'Arabic',\n",
    "    'hi': 'Hindi',\n",
    "    'te': 'Telugu'  # Added Telugu\n",
    "}\n",
    "\n",
    "# Load all the datasets into a list of DataFrames\n",
    "datasets = [\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Lifestyle-Related Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/disease_data.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Environmental Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Idiopathic.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Neoplastic Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/non-infectious diseases_data.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Nutritional Diseases.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Psychiatric and Neurological Disorders.csv\"),\n",
    "    pd.read_csv(\"D:/AI-Projects/FinalProject/P1/Rare Diseases.csv\")\n",
    "]\n",
    "\n",
    "# Combine all datasets into one DataFrame for easier searching\n",
    "combined_df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Clean and normalize text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())  # Remove non-alphanumeric characters and convert to lowercase\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean the Symptoms column in the combined DataFrame\n",
    "combined_df['Symptoms'] = combined_df['Symptoms'].apply(clean_text)\n",
    "\n",
    "# Translate text to English if not already in English\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        user_lang = detect(text)\n",
    "        if user_lang != 'en':\n",
    "            text = GoogleTranslator(source=user_lang, target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating to English: {e}\")\n",
    "    return text\n",
    "\n",
    "# Print disease information in a formatted way\n",
    "def print_disease_info(disease, language='en'):\n",
    "    fields = {\n",
    "        'Disease Name': disease['Disease Name'],\n",
    "        'Severity Level': disease['Severity Level'],\n",
    "        'Symptoms': disease['Symptoms'],\n",
    "        'Recommended Medications': disease['Recommended Medications'],\n",
    "        'Required Food': disease['Required Food'],\n",
    "        'Safety Precautions': disease['Safety Precautions'],\n",
    "        'Recommended Doctor': disease['Recommended Doctor'],\n",
    "        'Treatment Plan': disease['Treatment Plan'],\n",
    "        'Follow-Up Recommendations': disease['Follow-Up Recommendations'],\n",
    "        'Patient Education': disease['Patient Education'],\n",
    "        'Recovery Time': disease['Recovery Time']\n",
    "    }\n",
    "    \n",
    "    if language in SUPPORTED_LANGUAGES:\n",
    "        try:\n",
    "            fields = {key: GoogleTranslator(source='auto', target=language).translate(value) for key, value in fields.items()}\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating to {language}: {e}\")\n",
    "            fields = {key: value for key, value in fields.items()}\n",
    "    else:\n",
    "        print(f\"Language code '{language}' is not supported. Showing information in English.\")\n",
    "\n",
    "    for key, value in fields.items():\n",
    "        print(f\"{key:<25} {value}\")\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "# Identify disease based on symptoms\n",
    "def identify_disease(user_input, language='en'):\n",
    "    # Translate user input to English if needed\n",
    "    user_input = translate_to_english(user_input)\n",
    "\n",
    "    # Clean and extract symptoms from the user input\n",
    "    cleaned_input = clean_text(user_input)\n",
    "\n",
    "    # Combine the cleaned user input with the dataset symptoms for vectorization\n",
    "    all_symptoms = combined_df['Symptoms'].tolist() + [cleaned_input]\n",
    "\n",
    "    # Fit the TfidfVectorizer on the combined data\n",
    "    tfidf = TfidfVectorizer().fit_transform(all_symptoms)\n",
    "\n",
    "    # Separate the TF-IDF matrix for the dataset symptoms and the user's input\n",
    "    dataset_tfidf = tfidf[:-1]  # All rows except the last one\n",
    "    input_tfidf = tfidf[-1]  # The last row is the user's input\n",
    "\n",
    "    # Compute cosine similarity between the user's input and the dataset symptoms\n",
    "    cosine_similarities = cosine_similarity(input_tfidf, dataset_tfidf).flatten()\n",
    "\n",
    "    # Get the index of the highest similarity\n",
    "    best_match_index = cosine_similarities.argmax()\n",
    "\n",
    "    # Check if the highest similarity is below a certain threshold\n",
    "    if cosine_similarities[best_match_index] < 0.1:\n",
    "        print(\"No close match found in the dataset. Please check your input or try different symptoms.\")\n",
    "    else:\n",
    "        # Get the matching disease information\n",
    "        matching_disease = combined_df.iloc[best_match_index]\n",
    "        print_disease_info(matching_disease, language)\n",
    "\n",
    "# Convert MP3 to WAV format\n",
    "def convert_mp3_to_wav(mp3_file_path):\n",
    "    wav_file_path = mp3_file_path.replace('.mp3', '.wav')\n",
    "    try:\n",
    "        audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "        audio.export(wav_file_path, format='wav')\n",
    "        print(f\"Converted MP3 file to WAV format: {wav_file_path}\")\n",
    "        return wav_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting MP3 to WAV: {e}\")\n",
    "        return None\n",
    "\n",
    "# Record audio and save it to a file\n",
    "def record_audio(output_file_path):\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=44100, input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    # Record for a fixed duration of 5 seconds\n",
    "    for _ in range(int(44100 / 1024 * 5)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    \n",
    "    print(\"Recording finished.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    \n",
    "    try:\n",
    "        with wave.open(output_file_path, 'wb') as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "            wf.setframerate(44100)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "        print(f\"Recorded audio saved to {output_file_path}\")\n",
    "\n",
    "        # Recognize the audio and return the recognized text\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(output_file_path) as source:\n",
    "            audio = recognizer.record(source)\n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio, language='te-IN')  # Specify Telugu language code\n",
    "                print(f\"Recognized text: {text}\")\n",
    "                return text\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Sorry, could not understand the audio.\")\n",
    "                return \"\"\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Sorry, there was an error with the speech recognition service: {e}\")\n",
    "                return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or processing audio file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Get user input from a recorded audio file\n",
    "def get_recorded_audio_input(file_path):\n",
    "    file_path = file_path.strip('\"')\n",
    "    \n",
    "    if file_path.lower().endswith('.mp3'):\n",
    "        wav_file_path = convert_mp3_to_wav(file_path)\n",
    "        if wav_file_path is None:\n",
    "            return \"\"\n",
    "        file_path = wav_file_path\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        with sr.AudioFile(file_path) as source:\n",
    "            print(f\"Processing recorded audio file: {file_path}\")\n",
    "            audio = recognizer.record(source)\n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio, language='te-IN')  # Specify Telugu language code\n",
    "                print(f\"Recognized text: {text}\")\n",
    "                return translate_to_english(text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Sorry, could not understand the audio.\")\n",
    "                return \"\"\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Sorry, there was an error with the speech recognition service: {e}\")\n",
    "                return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def identify_disease_from_image(image_file_path):\n",
    "    try:\n",
    "        # Load the model\n",
    "        model = load_model('D:/AI-Projects/chest_xray/pneumonia_detection_model.h5')\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        img = keras_image.load_img(image_file_path, target_size=(150, 150))  # Ensure target size matches the model input\n",
    "        x = keras_image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)  # Adjust this if needed\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(x)\n",
    "        \n",
    "        # Interpret the prediction\n",
    "        # Assuming output is a probability; adjust threshold as needed\n",
    "        predicted_class = 'Pneumonia' if preds[0][0] > 0.5 else 'Normal'\n",
    "        \n",
    "        return predicted_class\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying disease from image: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Process image input and find disease information\n",
    "def process_image_input(image_file_path):\n",
    "    disease_label = identify_disease_from_image(image_file_path)\n",
    "    \n",
    "    if disease_label:\n",
    "        # Search the disease label in the combined dataset\n",
    "        matched_disease = combined_df[combined_df['Disease Name'].str.contains(disease_label, case=False, na=False)]\n",
    "        \n",
    "        if not matched_disease.empty:\n",
    "            for _, disease in matched_disease.iterrows():\n",
    "                print_disease_info(disease)\n",
    "        else:\n",
    "            print(f\"It Means Your X_Ray Was at Normal Condition: {disease_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe4d47-ca36-47e5-b5bf-aecce415ba31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
